We have trouble telling ourselves and each other what we want, so how can we expect to tell intelligent machines what we want? Preference elicitation is not easy for human beings. It won't be easy for intelligent machines, either.

# Why it is difficult to elicit human preferences  
 
 Human beings often do manage to learn what they and other human beings desire. How? "Economists elicit preferences from human subjects by offering them choices." For AIs, what we want is something like [[Inverse Reinforcement Learning]] (also known as "structural estimation of Markov decision processes", see Tom Sargent). In typical reinforcement learning, behaviors are generated from rewards. In inverse reinforcement learning (IRL), the idea is to learn the reward function from the behavior. If a machine can learn its human's reward function, it can also predict its human's behavior in new circumstances.  

Some challenges in eliciting preferences:  
  
- The choices we care about most are complex. You can imagine an exercise in which an AI asks you questions to discover your preferences regarding pizza toppings. Single choices made between objects whose desirability is obvious and immediate are comparatively easy. "It's not obvious how to extend it [this process of preference elicitation] to preferences between future lives." At minimum, we have to observe choices over time and account for uncertainty of outcomes.  
- Humans are irrational: we do not always make decisions that are in our best interest. We are often influenced by emotions, biases, etc. Our preferences are not always (are often not) consistent.  
- Human preferences are complex  
- Human preferences are dynamic. What we want today isn't necessarily what we'll want tomorrow.  
- Human preferences can be hidden  
- Human's make mistakes, so judging our preferences from our decisions and our actions must take into account our ignorance, etc. An intelligent machines would need a fairly complicated model of our beliefs to make good inferences about why we do what we do (but how would the machine learn about our beliefs except by seeing how we act?). 