[[Stuart Russell]] believes we erred in attempting to give intelligent machines **definite** goals. That is, we've mistakenly thought that machines are intelligent to the extent that their actions can be expected to achieve *their* objective. Truly beneficial intelligent machines would, through their actions, help us achieve *our* objectives. 

What's wrong with *definite* objectives? In short, we don't know how to precisely specify our objectives. Even if we could, our objectives can change.

[[Stuart Russell]] proposes, then, three principles for beneficial machines:
1. The machine's only objective is to maximize the realization of human preferences [^human-preferences]
2. The machine is initially uncertain about what those preferences are.
3. The ultimate source of information about human preferences is human behavior.

He's calling for perfect altruism and bedrock humility in intelligent machines as well as a recognition about the fuzziness and malleability in our preferences. [[Eliciting Preferences]] becomes a fundamental problem.


[^human-preferences]: According to Russell, "preferences are all-encompassing; they cover everything you might care about, arbitrarily far into the future. And they are yours: the machine is not looking to identify or adopt one ideal set of preferences but to understand and satisfy . . . the preferences of each person."